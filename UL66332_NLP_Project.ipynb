{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bd8dedbd-de52-46f6-8ece-6b661d2e35e1",
   "metadata": {},
   "source": [
    "## A. Environment Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd09d27f-aa65-4a3d-a72b-2bda486f80bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, random, torch, re\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "from torch.utils.data import Dataset\n",
    "from datasets import load_dataset, Dataset as HFDataset\n",
    "from PIL import Image\n",
    "from IPython.display import display\n",
    "import spacy\n",
    "\n",
    "\n",
    "from transformers import (\n",
    "    BlipProcessor, BlipForConditionalGeneration,\n",
    "    T5Tokenizer, T5ForConditionalGeneration,\n",
    "    CLIPProcessor, CLIPModel,\n",
    "    TrainingArguments, Trainer, TrainerCallback, EvalPrediction,\n",
    "    default_data_collator, pipeline\n",
    ")\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "from nltk.translate.meteor_score import meteor_score\n",
    "from rouge import Rouge\n",
    "\n",
    "os.environ[\"HF_HOME\"] = \"D:/NLP_Cache\"\n",
    "os.environ[\"HF_DATASETS_CACHE\"] = \"D:/NLP_Cache/datasets\"\n",
    "os.environ[\"TRANSFORMERS_CACHE\"] = \"D:/NLP_Cache/models\"\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "rouge_evaluator = Rouge()\n",
    "summarizer = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\")\n",
    "print(\"\\ud83d\\ude80 Using device:\", device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af241074-9e85-472d-be59-66ee117479c0",
   "metadata": {},
   "source": [
    "## B. Dataset Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cef2d47-f9a8-4e76-8646-2cb3b8934dba",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"Supermaxman/esa-hubble\", cache_dir=\"D:/NLP_Cache/datasets\")[\"train\"]\n",
    "print(\"Total records - \", len(dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5414a3f-4279-40f6-9bdb-efa3c89e422b",
   "metadata": {},
   "source": [
    "## C. Model and Tokenizer Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27e7064e-be49-45ee-a088-ccd4ff5b5932",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BlipForConditionalGeneration\n",
    "blip_model = BlipForConditionalGeneration.from_pretrained(\n",
    "    \"Salesforce/blip-image-captioning-base\"\n",
    ").to(device)\n",
    "\n",
    "blip_model     = BlipForConditionalGeneration.from_pretrained(\n",
    "    \"Salesforce/blip-image-captioning-base\"\n",
    ").to(device)\n",
    "\n",
    "blip_processor = BlipProcessor.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
    "\n",
    "# T5 Refinement Model\n",
    "t5_model = T5ForConditionalGeneration.from_pretrained(\"google/flan-t5-base\").to(device)\n",
    "t5_tokenizer = T5Tokenizer.from_pretrained(\"google/flan-t5-base\")\n",
    "\n",
    "# CLIP for similarity\n",
    "clip_processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "clip_model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\").to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89d4778b-7b96-43a6-be22-5718e9acb59f",
   "metadata": {},
   "source": [
    "## SECTION 4: DATASET WRAPPER "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51017fe8-67d4-4b52-8028-a154b8cd8610",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HubbleBLIPDataset(Dataset):\n",
    "    def __init__(self, dataset, processor, max_samples=None):\n",
    "        self.dataset = dataset.select(range(max_samples)) if max_samples else dataset\n",
    "        self.processor = processor\n",
    "\n",
    "    def __len__(self): return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample = self.dataset[idx]\n",
    "        image = sample[\"image\"]\n",
    "        if isinstance(image, list): image = image[0]\n",
    "        image = image.convert(\"RGB\")\n",
    "        caption = str(sample.get(\"description\", \"\"))\n",
    "        prompt = \"Describe the astronomical image.\"\n",
    "\n",
    "        inputs = self.processor(images=image, text=prompt, return_tensors=\"pt\",\n",
    "                                padding=\"max_length\", truncation=True, max_length=64)\n",
    "        labels = self.processor.tokenizer(caption, return_tensors=\"pt\", padding=\"max_length\",\n",
    "                                          truncation=True, max_length=64)[\"input_ids\"].squeeze(0)\n",
    "        inputs = {k: v.squeeze(0) for k, v in inputs.items()}\n",
    "        inputs[\"labels\"] = labels\n",
    "        return inputs\n",
    "\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "import torch\n",
    "\n",
    "class HubbleBLIPDataset(Dataset):\n",
    "    def __init__(self, dataset, processor, max_samples=None):\n",
    "        self.dataset = dataset.select(range(max_samples)) if max_samples else dataset\n",
    "        self.processor = processor\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample = self.dataset[idx]\n",
    "\n",
    "        image = sample[\"image\"]\n",
    "        if isinstance(image, list):\n",
    "            image = image[0]\n",
    "        image = image.convert(\"RGB\")\n",
    "\n",
    "        caption = str(sample.get(\"description\", \"\"))\n",
    "        prompt = \"Describe the astronomical image.\"\n",
    "\n",
    "        inputs = self.processor(\n",
    "            images=image,\n",
    "            text=prompt,\n",
    "            return_tensors=\"pt\",\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            max_length=64,\n",
    "        )\n",
    "\n",
    "        labels = self.processor.tokenizer(\n",
    "            caption,\n",
    "            return_tensors=\"pt\",\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            max_length=64,\n",
    "        )[\"input_ids\"].squeeze(0)\n",
    "\n",
    "        inputs = {k: v.squeeze(0) for k, v in inputs.items()}\n",
    "        inputs[\"labels\"] = labels\n",
    "\n",
    "        return inputs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00e095b3-f816-4c71-a167-8d6704ecfe9d",
   "metadata": {},
   "source": [
    "## E. Data Splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9811598-c2bb-41b1-9196-f2a5ef8454cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from datasets import Dataset\n",
    "\n",
    "# Split Hugging Face dataset into train and eval sets (e.g., 90/10 split)\n",
    "train_data = dataset.select(range(int(len(dataset) * 0.9)))\n",
    "eval_data  = dataset.select(range(int(len(dataset) * 0.9), len(dataset)))\n",
    "\n",
    "# Prepare datasets\n",
    "train_dataset = HubbleBLIPDataset(train_data, blip_processor, max_samples=450)\n",
    "eval_dataset  = HubbleBLIPDataset(eval_data, blip_processor, max_samples=50)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a345efe-fd46-4756-adad-2bed0ee429db",
   "metadata": {},
   "source": [
    "## F. Named Entity Recognition and Object ID Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40836587-81d2-4dcd-9872-642ad4b3397f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "spacy_model = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "def extract_named_entities(text):\n",
    "    doc = spacy_model(text)\n",
    "    entities = defaultdict(list)\n",
    "    for ent in doc.ents:\n",
    "        entities[ent.label_].append(ent.text)\n",
    "    return dict(entities)\n",
    "\n",
    "def extract_astronomical_entities(text):\n",
    "    pattern = r\"\\b(?:NGC|UGC|IC|M|Messier)\\s?\\d{1,5}\\b\"\n",
    "    return re.findall(pattern, text.upper())\n",
    "\n",
    "def extract_object_ids(text):\n",
    "    return re.findall(r'\\b(NGC|UGC|IC|M)\\s?\\d{2,5}\\b', text.upper())\n",
    "\n",
    "def link_object_ids(text, known_ids):\n",
    "    return [obj for obj in known_ids if obj.lower() in text.lower()]\n",
    "\n",
    "def summarize_text(text, max_length=60, min_length=20):\n",
    "    try:\n",
    "        summary = summarizer(text, max_length=max_length, min_length=min_length, do_sample=False)\n",
    "        return summary[0][\"summary_text\"]\n",
    "    except Exception as e:\n",
    "        print(f\"âš ï¸ Summarization failed: {e}\")\n",
    "        return \"[Summary unavailable]\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f28ef4b4-5019-43c9-bb24-44eed1858ad4",
   "metadata": {},
   "source": [
    "## G. Caption Generation and Refinement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b035668-cba8-43c8-99b9-3e6913f47021",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_blip_caption(image):\n",
    "    image = image.convert(\"RGB\").resize((224, 224))\n",
    "    inputs = blip_processor(images=image, return_tensors=\"pt\").to(device)\n",
    "    outputs = blip_model.generate(**inputs, max_length=50, repetition_penalty=1.2, no_repeat_ngram_size=3)\n",
    "    return blip_processor.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "def refine_caption(blip_caption: str, beams: int = 4):\n",
    "    if not blip_caption.strip(): return \"No caption generated.\"\n",
    "    prompt = f\"Rewrite the following astronomy image caption so it is concise, scientifically correct, and fluent:\\n{blip_caption.strip()}\"\n",
    "    inputs = t5_tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "    outputs = t5_model.generate(**inputs, num_beams=beams, early_stopping=True,\n",
    "                                max_length=40, length_penalty=0.8,\n",
    "                                no_repeat_ngram_size=3, repetition_penalty=1.2)\n",
    "    return t5_tokenizer.decode(outputs[0], skip_special_tokens=True).strip()\n",
    "\n",
    "def generate_caption_with_metadata(image, metadata):\n",
    "    title = metadata.get('title', '').strip()\n",
    "    constellation = metadata.get('Constellation', '').strip()\n",
    "    category = metadata.get('Category', '').strip()\n",
    "    distance = metadata.get('Distance', '').strip()\n",
    "    parts = []\n",
    "    if title: parts.append(f\"of '{title}'\")\n",
    "    if constellation: parts.append(f\"located in the constellation {constellation}\")\n",
    "    if category: parts.append(f\"which belongs to the category {category}\")\n",
    "    if distance: parts.append(f\"and is {distance} away\")\n",
    "    prompt = \"Describe the astronomical image \" + \", \".join(parts) + \".\"\n",
    "    inputs = blip_processor(images=image, text=prompt, return_tensors=\"pt\").to(device)\n",
    "    outputs = blip_model.generate(**inputs, max_length=50)\n",
    "    return blip_processor.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "def refine_caption_with_metadata(caption, metadata):\n",
    "    prompt = (f\"Refine this caption using astronomy terms. Context: title: {metadata.get('title', '')}, \"\n",
    "              f\"constellation: {metadata.get('Constellation', '')}, \"\n",
    "              f\"category: {metadata.get('Category', '')}. Caption: {caption}\")\n",
    "    inputs = t5_tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "    outputs = t5_model.generate(**inputs, max_length=60)\n",
    "    return t5_tokenizer.decode(outputs[0], skip_special_tokens=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a13f95d-3cc6-44ca-b1bf-402247c9692f",
   "metadata": {},
   "source": [
    "## H. CLIP-Based Caption Reranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6cf3112-8dd7-470c-917f-2fc775f6d230",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_clip_score(image, caption):\n",
    "    inputs = clip_processor(text=[caption], images=image, return_tensors=\"pt\", padding=True, truncation=True).to(device)\n",
    "    with torch.no_grad():\n",
    "        outputs = clip_model(**inputs)\n",
    "    return cosine_similarity(outputs.image_embeds.cpu().numpy(), outputs.text_embeds.cpu().numpy())[0][0]\n",
    "\n",
    "def clip_rerank(image, captions):\n",
    "    best_score = -1\n",
    "    best_caption = \"\"\n",
    "    for caption in captions:\n",
    "        inputs = clip_processor(text=[caption], images=image, return_tensors=\"pt\", padding=True, truncation=True).to(device)\n",
    "        with torch.no_grad():\n",
    "            outputs = clip_model(**inputs)\n",
    "        score = cosine_similarity(outputs.image_embeds.cpu().numpy(), outputs.text_embeds.cpu().numpy())[0][0]\n",
    "        if score > best_score:\n",
    "            best_score = score\n",
    "            best_caption = caption\n",
    "    return best_caption, best_score\n",
    "\n",
    "\n",
    "def compute_clip_similarity(image_emb, text_emb):\n",
    "    return cosine_similarity(image_emb.cpu().detach().numpy(), text_emb.cpu().detach().numpy())[0][0]\n",
    "\n",
    "def prepare_clip_guided_rerank_options(captions, image, clip_processor, clip_model, device=\"cuda\"):\n",
    "    similarities = []\n",
    "    for caption in captions:\n",
    "        inputs = clip_processor(text=[caption], images=image, return_tensors=\"pt\", padding=True, truncation=True).to(device)\n",
    "        with torch.no_grad():\n",
    "            outputs = clip_model(**inputs)\n",
    "        sim = compute_clip_similarity(outputs.image_embeds, outputs.text_embeds)\n",
    "        similarities.append(sim)\n",
    "    best_index = int(torch.tensor(similarities).argmax())\n",
    "    return captions[best_index], similarities[best_index]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6b53e6a-a281-4a00-9750-8652dbdd1665",
   "metadata": {},
   "source": [
    "## I.Caption Evaluation Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6c8d537-1f06-4a9c-b483-ffbef00d8e01",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_caption_metrics(eval_pred: EvalPrediction):\n",
    "    predictions, labels = eval_pred\n",
    "    if isinstance(predictions, tuple): predictions = predictions[0]\n",
    "    pred_ids = np.argmax(predictions, axis=-1)\n",
    "    decoded_preds = blip_processor.tokenizer.batch_decode(pred_ids, skip_special_tokens=True)\n",
    "    decoded_labels = blip_processor.tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "    smoothie = SmoothingFunction().method4\n",
    "    scores = {\n",
    "        \"BLEU\": np.mean([sentence_bleu([ref.split()], pred.split(), smoothing_function=smoothie)\n",
    "                         for pred, ref in zip(decoded_preds, decoded_labels)]),\n",
    "        \"ROUGE-L\": np.mean([rouge_evaluator.get_scores(pred, ref)[0][\"rouge-l\"][\"f\"]\n",
    "                            for pred, ref in zip(decoded_preds, decoded_labels)]),\n",
    "        \"METEOR\": np.mean([meteor_score([ref.split()], pred.split())\n",
    "                           for pred, ref in zip(decoded_preds, decoded_labels)])\n",
    "    }\n",
    "    return scores\n",
    "\n",
    "def evaluate_captions(reference, generated):\n",
    "    smoothie = SmoothingFunction().method4\n",
    "    bleu = sentence_bleu([reference.split()], generated.split(), smoothing_function=smoothie)\n",
    "    rouge = rouge_evaluator.get_scores(generated, reference)[0][\"rouge-l\"][\"f\"]\n",
    "    meteor = meteor_score([reference.split()], generated.split())\n",
    "    return {\"BLEU\": bleu, \"ROUGE-L\": rouge, \"METEOR\": meteor}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df1bd6d1-952a-4d41-a89c-dd55199c43a2",
   "metadata": {},
   "source": [
    "## J. Training Loop and Callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c287eebe-3ea4-451f-8e6e-6a04c825c845",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PrintStepCallback(TrainerCallback):\n",
    "    def on_log(self, args, state, control, logs=None, **kwargs):\n",
    "        if logs: print(f\"Step {state.global_step} - Logs: {logs}\")\n",
    "\n",
    "class PrintEpochCallback(TrainerCallback):\n",
    "    def on_epoch_end(self, args, state, control, **kwargs):\n",
    "        print(f\"ðŸ“¢ Finished Epoch {int(state.epoch)}\")\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./blip_finetuned_light\",\n",
    "    per_device_train_batch_size=2,\n",
    "    num_train_epochs=4,\n",
    "    gradient_accumulation_steps=4,\n",
    "    learning_rate=5e-5,\n",
    "    warmup_steps=200,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    logging_strategy=\"steps\",\n",
    "    logging_steps=10,\n",
    "    fp16=torch.cuda.is_available(),\n",
    "    save_total_limit=2,\n",
    "    remove_unused_columns=False,\n",
    "    report_to=\"none\",\n",
    "    metric_for_best_model=\"eval_METEOR\",\n",
    "    greater_is_better=True\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=blip_model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    tokenizer=blip_processor.tokenizer,\n",
    "    data_collator=default_data_collator,\n",
    "    compute_metrics=compute_caption_metrics,\n",
    "    callbacks=[PrintEpochCallback(), PrintStepCallback()]\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1311164f-55b1-4fd2-beb3-f880f2389f61",
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = random.sample(range(len(dataset)), 5)\n",
    "\n",
    "for idx in indices:\n",
    "    image = dataset[idx][\"image\"]\n",
    "    if image is None:\n",
    "        continue\n",
    "\n",
    "    reference = dataset[idx][\"description\"]\n",
    "    blip_caption = generate_blip_caption(image)\n",
    "    refined = refine_caption(blip_caption)\n",
    "    reranked, _ = clip_rerank(image, [blip_caption, refined])\n",
    "    clip_blip = get_clip_score(image, refined)\n",
    "    clip_ref = get_clip_score(image, reference)\n",
    "    metrics = evaluate_captions(reference, refined)\n",
    "    entities = extract_named_entities(reference)\n",
    "    summary = summarize_text(reference)\n",
    "\n",
    "    print(f\"\\n Image Index - {idx}\")\n",
    "    display(image)\n",
    "    print(f\"BLIP Caption - {blip_caption}\")\n",
    "    print(f\"Refined Caption - {refined}\")\n",
    "    print(f\"CLIP-Reranked Caption - {reranked}\")\n",
    "    print(f\"Reference - {reference}\")\n",
    "    print(f\"Summary (T5) - {summary}\")\n",
    "    print(f\"Named Entities - {entities}\")\n",
    "    print(f\"CLIP Refined - {clip_blip:.4f} | CLIP (Reference): {clip_ref:.4f}\")\n",
    "    print(\"NLP Metrics -\", metrics)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0e07389-f36e-4bdd-80dc-df47ae246894",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
